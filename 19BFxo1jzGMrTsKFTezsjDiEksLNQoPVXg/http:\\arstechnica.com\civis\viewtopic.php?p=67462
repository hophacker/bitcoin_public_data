http://arstechnica.com/civis/viewtopic.php?p=67462
The AudioFile: basics of uncompressed digital audio - Ars Technica OpenForum
Welcome to the Ars OpenForum.
Register
Login
Posting Guidelines | Contact Moderators
Ars Technica > Forums
Jump to:
Select a forum
------------------
Hardware & Tweaking
Audio/Visual Club
Case and Cooling Fetish
CPU & Motherboard Technologia
Mobile Computing Outpost
Networking Matrix
Other Hardware
Agora Classifieds
Operating Systems & Software
Battlefront
Microsoft OS & Software Colloquium
Linux Kung Fu
Windows Technical Mojo
Distributed Computing Arcana
Macintoshian Achaia
Programmer's Symposium
The Server Room
Ars Lykaion
Gaming, Extra Strength Caplets
The Lounge
The Soap Box
The Boardroom
The Observatory
Ars Help & Feedback
Ars Subscription Member Areas
Image Galleries
The AudioFile: basics of uncompressed digital audio
254 posts • 123 ... 7 Next
JournalBot
Ars Legatus Legionis
et Subscriptor
Registered: Apr 5, 2005Posts: 60134
Posted: Tue Sep 18, 2007 10:40 pm
Bit rate, bit depth, sample rate... what's the difference? Part one of a series on digital audio gives you the scoop on the basics of digital audio in its most common, uncompressed form.Read More
redleader
Ars Legatus Legionis
Registered: Mar 18, 2000Posts: 26072
Posted: Tue Sep 18, 2007 11:15 pm
Good to see a well written explanation. Now if only someone would update HowStuffWorks and bring their sampling theory stuff into the early 20th century That said, one point:quote: Another reason to go 24-bit is that it offers better bass definition. Why? Low frequency sounds, like the picture below, have very long periods by definition, meaning that they change slowly compared to higher frequencies. At lower precision levels, the signal must be rounded up by a greater amount in order to match the closest value. By adding precision, the digital signal more smoothly approximates the signal's change over time, which we hear as cleaner, more precise low end. Well, assuming you listened to a pure low frequency tone with no harmonics. This doesn't seem like a real problem though, when was the last time anyone found music that consisted of a pure, steady state sin wave? As soon as you add a second frequency, you're decorrelating the signal error between adjacent samples, and thus eliminating this problem. And anyway, this is an argument for dithering (which removes the correlation), not larger sample size (which merely hides it).
Jeremy W
Ars Scholae Palatinae
et Subscriptor
Registered: Nov 29, 2006Posts: 634
Posted: Wed Sep 19, 2007 12:23 am
Great article! It's all review for me, but I think you did a great job breaking everything down and explaining it. I'm looking forward to future installments!
owdi
Ars Praefectus
Tribus: Kenmore, WA
Registered: Feb 18, 2002Posts: 3453
Posted: Wed Sep 19, 2007 12:44 am
quote: Another reason to go 24-bit is that it offers better bass definition. Why? Low frequency sounds, like the picture below, have very long periods by definition, meaning that they change slowly compared to higher frequencies. At lower precision levels, the signal must be rounded up by a greater amount in order to match the closest value. By adding precision, the digital signal more smoothly approximates the signal's change over time, which we hear as cleaner, more precise low end. This explanation does not make sense. Increased bit depth benefits all frequencies equally. Low frequencies, if anything, benefit less, because the human ear is much less sensitive to them.Dan
Major General Thanatos
"The Very Model of"
Ars Legatus Legionis
et Subscriptor
Tribus: Czar of Meaningful Contribution Decisions
Registered: Dec 1, 2004Posts: 18984
Posted: Wed Sep 19, 2007 12:45 am
Nicely done. I used to know this information, very nice to have a refresher. I'll be definitely interested in reading the future installments. Thank you.
andyfatbastard
"lol donkaments"
Ars Legatus Legionis
et Subscriptor
Tribus: 19BFxo1jzGMrTsKFTezsjDiEksLNQoPVXg
Registered: Oct 17, 2000Posts: 19944
Posted: Wed Sep 19, 2007 1:26 am
/me is looking forward to a readable explanation of Euler's Theorem as it relates to digital signal processing. Peter Kasting
Wise, Aged Ars Veteran
Registered: Jul 26, 2004Posts: 189
Posted: Wed Sep 19, 2007 2:26 am
Not every form of digital representation uses absolute values for their samples. What about things like DSD, from SACD?
RogerGraham
Wise, Aged Ars Veteran
Registered: Apr 28, 2003Posts: 183
Posted: Wed Sep 19, 2007 2:56 am
quote:Originally posted by Peter Kasting:Not every form of digital representation uses absolute values for their samples. What about things like DSD, from SACD? Exactly. And regarding this:"What is important to know is that PCM is ... usually the highest quality"I am not an expert, but I am led to understand that DSD is regarded as a far better solution for archiving than PCM?
MachFour
Ars Tribunus Militum
Registered: May 18, 2005Posts: 2038
Posted: Wed Sep 19, 2007 3:55 am
Not a bad article, but what is really lacking is the consideration of the DAC. An all too popular misconception about digital audio is that somehow some people are able to hear the "discreteness" of the sampling if their ears are sensitive enough, probably induced by the somewhat misleading pictures of the discrete samples. I think it's important to emphasize that the signal that is sent to the speakers from a CD player is as smooth and continuous as that from an analog source like a turntable. It's nice to have pictures of the ADC samples of the signal and to make the point that higher resolutions are more accurate, but I think it's just as important to include pictures of the output from the DAC to show that you never hear the sampled discrete signals. In fact, it's probably even more enlightening to show pictures of the DAC output from a low resolution sample to drive home the point that even at low sampling resolutions, what gets sent to the speaker is always a smooth continuous signal and never the staircase as many people seem to believe. (The main difference is that analog form reconstructed from a low res sample is less accurate, but it's not "more discrete")
vershner
Wise, Aged Ars Veteran
Registered: Sep 5, 2006Posts: 142
Posted: Wed Sep 19, 2007 4:05 am
Nice article, a good refresher on audio basics. One minor quibble is that I don't think a NES is a very good example of 8-bit sound. The NES had quite a lot of other limitations on the sound it could produce. A better example would be an Amiga or Atari STE. (8-bit 22kHz) or a telephone (8-bit 8kHz).
raxx7
Ars Scholae Palatinae
Registered: Jul 17, 2006Posts: 963
Posted: Wed Sep 19, 2007 4:08 am
quote:Originally posted by RogerGraham:quote:Originally posted by Peter Kasting:Not every form of digital representation uses absolute values for their samples. What about things like DSD, from SACD? Exactly. And regarding this:"What is important to know is that PCM is ... usually the highest quality"I am not an expert, but I am led to understand that DSD is regarded as a far better solution for archiving than PCM? There is a lot argument about the merits of DSD vs PCM, from the theoretical limitations (bitrate vs quality) to the practial considerations (realistic A/D and D/A conversion).However, it's mostly academic. SACD is the only system I know of that uses DSD. Everything else uses PCM.
raxx7
Ars Scholae Palatinae
Registered: Jul 17, 2006Posts: 963
Posted: Wed Sep 19, 2007 4:31 am
I didn't like the article. I understand that it tries to address those who don't know much about it but it looks sloppy and confusing.In particular, I dislike the part that says that more bits allow for better bass response.To the best of my knowledge, PCM's quatization noise has a uniform spectrum -- it kind of looks like white noise with ocasional spikes.Increasing the bit depth will improve the dynamic range and noise floor for all frequencies.However, music produced by humans often has non-uniform spectrum and increasing the dynamic range for low/medium frequencies is specially interesting.Assuming, of course, the music hasn't had it's dynamic range compressed to hell.
robotic_tourist
Ars Tribunus Militum
Registered: May 9, 2005Posts: 2541
Posted: Wed Sep 19, 2007 4:42 am
I know I did a half-module on communications theory at university, but this article seemed a little light to me. Having said that, it is good to read over this sort of stuff from time to time, e.g. I knew about Shannon-Nyquist from my course (4+ years ago now), but I probably wouldn't have been able to recall it completely.++MachFour's suggestion about a little real world look at ADCs and DACs for comparison to the theory.It's good to know this is a series though and I am definitely looking forward to more details on how MP3/lossy sound compression works.
costique
Ars Scholae Palatinae
Registered: Jan 21, 2003Posts: 796
Posted: Wed Sep 19, 2007 5:52 am
There's also floating-point PCM which encodes samples with 32/64-bit values in the range from -1 to +1. What I don't know is how it defines actual amplitudes. Anybody care to explain, please?
Electron Pimp
Smack-Fu Master, in training
Registered: Mar 22, 2007Posts: 59
Posted: Wed Sep 19, 2007 6:27 am
It was a good read for a person like me who's not audio-inclined. I understood it well from the ADC measurement point of view. A similar explanation can be found in industrial vibration measurement papers.Does anyone else think the graphic representation of the circular pressure waves from the guitar is backwards? I would expect the amplitude of the sine wave to be higher closer to the source and lower the further away it gets. Am I off base?I'll definitely read more, especially if the DAC side of it comes up next (or more regarding surround sound recording and playback).
norton_I
Ars Praefectus
Registered: Apr 30, 2001Posts: 3397
Posted: Wed Sep 19, 2007 6:29 am
Yeah, when the article mentions that much software uses 32 or 64 bit internal representation, these are usually floating point values, not fixed. I don't know what the 'standard' floating point representation is, but I would guess that they usually scale 1.0 = maximum input sample. It doesn't really matter, since even a single-precision float has plenty of extra dynamics range (+/- 40 dB while maintaining a full 24 bits of vertical resolution).Also, as others have mentioned, with standard PCM encoding, increased vertical resolution does not help bass any more or less than higher frequencies.
dedsmith
Ars Scholae Palatinae
Registered: Dec 4, 2006Posts: 678
Posted: Wed Sep 19, 2007 6:47 am
quote:Originally posted by costique:There's also floating-point PCM which encodes samples with 32/64-bit values in the range from -1 to +1. What I don't know is how it defines actual amplitudes. Anybody care to explain, please? I was wondering about floating-point myself as I was reading the article. If human hearing has a logarithmic response (as I expect it does), I'd think a floating point format would be a more natural fit to the data.For example, in the range (-1,1) with single precision floats we have a 23-bit mantissa resulting in:+-(0.5,1) 8,388,608 levels+-(0.25,0.5) 8,388,608 levels+-(0.125,0.25) 8,388,608 levels+-(0.0625,0.125) 8,388,608 levels...As the intensities get lower, we have finer steps where the ear is more sensitive to smaller changes in intensity.Or am I way off base (pun intended here?
foxyshadis
Ars Scholae Palatinae
Registered: Sep 28, 2006Posts: 765
Posted: Wed Sep 19, 2007 6:47 am
quote:Originally posted by raxx7:However, music produced by humans often has non-uniform spectrum and increasing the dynamic range for low/medium frequencies is specially interesting.Assuming, of course, the music hasn't had it's dynamic range compressed to hell. In a way, psychoacoustic tunings of lossy audio might is roughly analogous to a per-frequency dynamic range, where some frequencies are quantized stronger, but on a finer scale. The quantization noise sounds different in frequency vs time domain, but it's the same sort of artifact.
DRVSVS
Smack-Fu Master, in training
Registered: Aug 11, 2007Posts: 65
Posted: Wed Sep 19, 2007 6:53 am
After reading the article I have a few questions:Is a musical signal only definable by its frequency, or does it also have tonal qualities? Are there differences between a saw, sine and square waves at a given frequency? Are these kind of waves used in music, and if so, where?Is there a difference between a sine and a saw wave of 11.025 kHz sampled at CD quality, when the wave is running in perfect sync with the ADC? If there is a difference, can humans perceive it, 11.025 kHz being well in range of most persons hearing? If the wave is out of sync to the ADC, will the amplitude of the signal be preserved? If the frequency of the signal is slightly above or below 11.025 khZ, what happens with the signal, compared to the signal sampled in perfect sync?How well is the nature of a sine wave of 5.5125 khZ represented at CD quality? If the sine wave should be slightly modulated (the signal having been produced by e.g. an FM synthesizer), would these slight differences be caught by sampling in CD quality? Is it inconceivable that some persons might well hear a difference between an analog/high-quality digital signal and CD-quality signal for such waveforms?Have you ever written music in a modern music production program, and mastered it using some kind of bitmeter-plugin for headroom monitoring? Was your track dynamic in the dimension of loudness? What problems did you encounter during mastering? How hard was it to actually use the full headroom without introducing clipping? How many bits did the "normally loud" passages use? If your track had actual dynamics and wasn't compressed to death, how many bits did the signal of the more soft parts of the track use? How many discrete loudness levels would this be? Did you try again using 24-bit resolution? How was your mastering experience influenced, did any answers to the above questions change?Have you ever played with analog synthesizer VST plugins, experimenting with different sampling frequencies for your host and/or changing oversampling settings for the VSTs? Did you perceive any change in tonal quality, especially when playing high notes? If there was any change, how would you describe it? Where would you put the possible source for the change?
DeadCat
Ars Scholae Palatinae
Registered: Aug 9, 2000Posts: 941
Posted: Wed Sep 19, 2007 6:56 am
quote:They may record at 88.2KHz or higher, which can capture frequencies up to 44,100Hz—more than twice the range of human hearing. Personally, I'm skeptical, since the physics of it are unlikely. Good, be skeptical of the physics, but don't knock an electrical engineer, man. It's called multi-rate sampling.If you know anything about sampling, you know that higher frequencies (above Nyquist/Shannon) alias back down into lower frequencies. Therefore, in front of the ADC is usually a low-pass anti-aliasing filter, which prevents the higher frequencies from folding back into sampled frequencies.Analog filters are crappy; they have pass-band ripple, stop-band ripple, imperfect stop-band attenuation, and a lengthy transition band. If you sample at 48 kHz, then your analog filter must have a transition band of about 4 kHz. This is pretty rough.If you sample at 96 kHz, your analog filter can now have a transition band of 28 kHz. You could then go with a lower order filter, or you could possibly use the extra transition slack to design a filter with better pass-band or stop-band characteristics.Then, once the data is in pristine digital format, it can be passed through a digital low-pass filter to clip everything above 20 kHz, and then you can decimate the sampling rate back down to something "reasonable" like 48 kHz.So, yes, recording engineers should insist that they hear something different between the sampling rates, because they're hearing the effect of better analog filters.quote:You can never get back the frequencies beyond the Nyquist-Shannon barrier Not strictly true. As above, those frequencies alias back into the sampled frequencies. If you design your anti-alias filter correctly, you can actually sample above Nyquist/Shannon - you have to set your anti-alias filter to band-pass the range of frequencies.I'm surprised they didn't mention that sound waves are longitudinal and not lateral, and that the waves are a series of rarefactions and compressions. Where's the physics lingo?Also, MAJOR error on page 2. "We could raise that to 4 bits, giving us 16 total values" - except the diagram shown is using 3 bits.I echo the call for details on the DAC. People think they observe the zeroth-order hold (where the DAC outputs the "step" waveform), but in reality that zeroth-order hold is passed through a reconstruction filter before going to the amplifiers.In fact, the reconstruction filter is exactly why the low frequency sound wave would sound the same even if you added bits.It might also be worth discussing sigma-delta architectures and how the ear works.EDIT: quote:I would guess that they usually scale 1.0 = maximum input sample Why? You're just giving yourself more work to do. There's no reason to scale the waveform, because then you have to un-scale it before you run it out the DAC.The integers from the ADC are "cast" into floats (without scaling), the processing is done with floats to maintain precision, and then the data is "cast" back to an integer before going to the DAC.
Mile Zero
"Goes to 11."
Ars Praetorian
et Subscriptor
Tribus: This space intentionally left blank.
Registered: Jul 7, 2005Posts: 536
Posted: Wed Sep 19, 2007 7:26 am
Deadcat: Yeah, I just caught the 3-bit illustration error myself. That's kind of a bummer. I'll see if I can get a replacement to Eric tonight--I can't access the Ars Features CMS directly yet.I thought about writing about the aliasing filters as an incentive to higher sample rates. I didn't for a couple of reasons, one of which is that I don't have much background in the EE side of things, and didn't want to make some huge mistake in my ignorance. Considering that, I probably shouldn't have made the snarky statement about my skepticism.I do plan on writing a future piece on 1-bit sampling that will necessarily cover that kind of information, and I promise I'll have my stuff together when I do.
vibedog
Ars Scholae Palatinae
Registered: Oct 27, 2005Posts: 913
Posted: Wed Sep 19, 2007 7:27 am
very nice article from ars. and also a lot of good comments that i mostly agree with but including them may raise the bar above the confusion level for the nontechnical audiophile. so i have nit as well, the digital vs. traditional photography was probably an okay analogy for the layperson, but traditional photography is not "analog", it is much closer to being digital than analog. the analogy is okay because digital photography has traditionally been of lower resolution than a photo produced in the darkroom, but that is changing now with 10 MPix cameras. i'm looking forward to the next installments. the way in which this information impacts nearly everybody who uses a computer is, when i put a cd in the drive to load into itunes, what format and sample rate do i use? i loaded my library onto the computer several years ago, and before doing so, i did a lot of "research" (googling) about which to use. i finally settled on 224 mp3, why? it wasn't apple specific, it was better than 128 (which is not cd quality in my non-audiophile opinion) but the file sizes were still a lot less than something like 384.
epp_b
Ars Tribunus Militum
Registered: Oct 22, 2005Posts: 1647
Posted: Wed Sep 19, 2007 7:32 am
Oooh...purty peekchas Flawed
Ars Tribunus Angusticlavius
Tribus: Still posting from a computer in Best Buy aisle 13.
Registered: Jan 14, 2000Posts: 9658
Posted: Wed Sep 19, 2007 7:51 am
Let me ask a silly question. Sound is really just a wave right? Waves can be graphed using mathematical functions right? Seems like there might be some mathematical function that can represent a given frequency. Given that, couldn't there be a method of representing analog sound digitally but without the stair steps? Not that it might have a practical purpose but from a theoretical standpoint it seems interesting.
DRVSVS
Smack-Fu Master, in training
Registered: Aug 11, 2007Posts: 65
Posted: Wed Sep 19, 2007 7:51 am
quote:Originally posted by norton_I:Yeah, when the article mentions that much software uses 32 or 64 bit internal representation, these are usually floating point values, not fixed. I don't know what the 'standard' floating point representation is, but I would guess that they usually scale 1.0 = maximum input sample. It doesn't really matter, since even a single-precision float has plenty of extra dynamics range (+/- 40 dB while maintaining a full 24 bits of vertical resolution). Steinberg uses floats from -1.0..1.0 in their VST standard, so most audio processing in music production on the Windows platform is done in this format. If a program supports VST at all, handling internal audio in this format will eliminate the need for data conversion when using VST effects. As VST processing is often done in real-time, not using -1.0..1.0 internally would be really bad for performance.As for internal processing, some VST developers actually do use integer processing, using 32bit fixed-point arithmetic for some purposes. This may be for performance reasons or also for quality - it seems as if some algorithms are better suited for integer processing. This will depend on the specific implementation of the algorithm used of course.More information on this can be found by browsing the KVR plug-in development forum. You might have to dig around to find the really interesting discussions; but there's a lot of good information about theory and practice of producing "good sound" to be found.
HolyChao
Ars Tribunus Militum
Tribus: S. Illinois - E. St. Louis
Registered: Nov 11, 2003Posts: 1592
Posted: Wed Sep 19, 2007 7:53 am
This appears, barring a few glitches in the original form, a damn good foundation for a reputable digital audio reference. (Any plans for a full Ars Whitepaper when it's done and .pdf availability? Never mind, I didn't look at the top of the article)
redleader
Ars Legatus Legionis
Registered: Mar 18, 2000Posts: 26072
Posted: Wed Sep 19, 2007 8:00 am
quote:Originally posted by Flawed:Let me ask a silly question. Sound is really just a wave right? Waves can be graphed using mathematical functions right? Seems like there might be some mathematical function that can represent a given frequency. There is, sin + cos.quote:Originally posted by Flawed:Given that, couldn't there be a method of representing analog sound digitally but without the stair steps? This is what PCM does. There aren't really steps. Theres a series of functions (sincs, which are a decaying sin wave) which are superimposed to create the output waveform. As you suspect, a finite number of these can be combined to produce a (finite bandwidth) analog waveform exactly.quote:Originally posted by Flawed: Not that it might have a practical purpose but from a theoretical standpoint it seems interesting. No, it has immense practical applications. Shannon's work in this area was one of the most important contributions to engineering of all time.
johnsonjohnson
Smack-Fu Master, in training
Registered: Oct 3, 2005Posts: 41
Posted: Wed Sep 19, 2007 8:02 am
First of all one reason to sample at frequencies greater than 44kHz is because you can use fewer bits in your A/D converter to get the same or superior resolution thanks to the magic of oversampling. Another reason to sample at greater than audio frequencies is because you can use filters to move noise to inaudible frequencies SACD uses only a 1 bit A/D converter but at a sample rate of 2.8224MHz. There's not much gain in going to 96kHz rates though, oversampling adds roughly the base 2 log of the oversampling rate bits so doubling your sample rate gets you at most one extra bit of resolution. ADC technology however has been on a Moore like technology curve of its own and its easy to get cheap parts that have both high bit depth and high sample rates so you can put in a 96kHz or 144kHz or whatever part and now have the freedom to shape the frequency content of the signal before decimation.quote:Originally posted by DRVSVS:After reading the article I have a few questions:Is a musical signal only definable by its frequency, or does it also have tonal qualities? Are there differences between a saw, sine and square waves at a given frequency? Are these kind of waves used in music, and if so, where? A musical signal or any audio signal is actually a function of both time and frequency since the same tones are not produced at all times. Think about a strumming a guitar; first there's no sound then there's the sound produced as you stroke the strings then there's the sound of the strings responding to your stroke which is diminishing in intensity as energy is transfered to the guitar body and air and finally there is not sound again. Audio engineers call the beginning and ending phases of such a signal the attack and decay. For a while the hot thing was to use wavelet representations of the signal but the analysis of such a 2D function is much harder than a simple 1D frequency only spectrum. There are two approximations that can be made though, the first is to chop up the time varying signal into equal sized windows and compute the spectrum in each window individually, if your windows are short enough then you can get a decent approximation of the original signal but not have to worry about the fact that it's a 1D projection of a 2D phenomenon. Otherwise you just transform the whole time varying signal and get effectively the projection of the 2D signal and don't worry about the fact that all frequencies in the spectrum are not actually present at all times, depending on what you are trying to do this may not be a problem.Your question about sine, square and saw waves is confusing the frequency and time representations. A pure tone is a time varying signal with sinusoid shape. Its frequency spectrum looks like two lines, or one line if you don't worry about negative frequencies. Square and saw forms have more complicated spectra but that doesn't change the analysis because each spectral component can be considered to correspond to a single sinusoidal signal at that frequency added to all the others. In music the spectral components less than that of greatest amplitude are considered overtones, and that's where the idea of tonality comes in. The spectrum of the signal from a musical instrument is never a single pure tone, in fact it is incredibly difficult to produce a pure tone, nearly any physical system is going to have audible overtones of some kind or another even a tuning fork. It's the presence of the overtones and their relationship to the fundamental, that is the frequency component of greatest magnitude, that make instruments sound different. So if you are wondering "where" these tones are they are in the particular characteristics that make one instrument sound differently than another.quote:Originally posted by DRVSVS:Is there a difference between a sine and a saw wave of 11.025 kHz sampled at CD quality, when the wave is running in perfect sync with the ADC? If there is a difference, can humans perceive it, 11.025 kHz being well in range of most persons hearing? If the wave is out of sync to the ADC, will the amplitude of the signal be preserved? If the frequency of the signal is slightly above or below 11.025 khZ, what happens with the signal, compared to the signal sampled in perfect sync? You can't have a saw wave of 11.025kHz because a saw wave has multiple frequency components. You can have a saw wave that has a fundamental frequency of 11.025kHz and yes it's different from a sine wave of the same fundamental frequency because it has overtones. And yes the saw wave will sound different because of the overtones as explained above.By the way you don't "sync" with the ADC. The ADC runs at a particular sample rate and that's it, the amplitude of the spectrum is unaffected but there's a phase term in the Fourier transform that's usually ignored in audio signals by looking at the power spectrum: the absolute value of the actual spectrum. Some people claim to be able to hear a phase shift but I don't believe them because there's no physical mechanism in a single ear that could measure it. On the other hand the phase difference between the signals measured at each ear is audible, it carries the information about the spatial location of the sound.quote:Originally posted by DRVSVS:How well is the nature of a sine wave of 5.5125 khZ represented at CD quality? If the sine wave should be slightly modulated (the signal having been produced by e.g. an FM synthesizer), would these slight differences be caught by sampling in CD quality? Is it inconceivable that some persons might well hear a difference between an analog/high-quality digital signal and CD-quality signal for such waveforms? A PCM coded signal has equal fidelity at all frequencies, the problem is in the noise floor. You have to specify how loud the signal is, not at what frequency to determine its relative fidelity, for example a -100dB signal is too quiet to be differentiated from noise at 16 bits although I believe a human ear should be able to detect it. But then we are wandering into the area of psychoacoustics, because our ears don't do PCM encoding and relative volumes matter more than absolute volumes, which is why you can't hear a whisper when someone else is yelling at you. On top of that our ears do not have a flat frequency response curve so a 5kHz signal may sound quieter or louder than a 15kHz signal at the same power level.quote:Originally posted by DRVSVS:Have you ever written music in a modern music production program, and mastered it using some kind of bitmeter-plugin for headroom monitoring? Was your track dynamic in the dimension of loudness? What problems did you encounter during mastering? How hard was it to actually use the full headroom without introducing clipping? How many bits did the "normally loud" passages use? If your track had actual dynamics and wasn't compressed to death, how many bits did the signal of the more soft parts of the track use? How many discrete loudness levels would this be? Did you try again using 24-bit resolution? How was your mastering experience influenced, did any answers to the above questions change? Sounding "compressed" is a psychological phenomenon. Clipping, which leads to something sounding "compressed" but which is an objectively measurable phenomenon, occurs because your recording level is too high, not because you don't have enough bits. Naively the recording level should be set such that the loudest signal in the entire piece exactly fills the ADC. More bits then would make things sound "richer" because they can more accurately capture the dynamics of the relative difference of quiet and loud signals. quote:Originally posted by DRVSVS:Have you ever played with analog synthesizer VST plugins, experimenting with different sampling frequencies for your host and/or changing oversampling settings for the VSTs? Did you perceive any change in tonal quality, especially when playing high notes? If there was any change, how would you describe it? Where would you put the possible source for the change? I haven't played this game. Oversampling gives more effective bit depth (see above), depending on the nature of your signal processing system it may also help with changing the tonal quality of signals with a lot of high frequency content (high notes) because you have to meet the Nyquist-Shannon sampling criterion for all the overtones. Then again because these overtones are supposedly inaudible we are wandering into the area of psychoacoustics again.
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:03 am
Aside from the notes about better bass performance being questionable, I will point out a few other things...quote:Originally posted by dedsmith:quote:Originally posted by costique:There's also floating-point PCM which encodes samples with 32/64-bit values in the range from -1 to +1. What I don't know is how it defines actual amplitudes. Anybody care to explain, please? I was wondering about floating-point myself as I was reading the article. If human hearing has a logarithmic response (as I expect it does), I'd think a floating point format would be a more natural fit to the data.For example, in the range (-1,1) with single precision floats we have a 23-bit mantissa resulting in:+-(0.5,1) 8,388,608 levels+-(0.25,0.5) 8,388,608 levels+-(0.125,0.25) 8,388,608 levels+-(0.0625,0.125) 8,388,608 levels...As the intensities get lower, we have finer steps where the ear is more sensitive to smaller changes in intensity.Or am I way off base (pun intended here? No you are pretty well on base actually. You can use floats from -1.0 to +1.0 to represent audio certainly, and it does a pretty good job of it as well. To answer the original question on how it represents amplitude, just like that. The range is defined from -1 to +1 and the difference between successive values is what determines the amplitude, much like in Integer PCM.On top of that, it uses larger bit-depth not only to increase the dynamic range, but also to allow people to drive any stage except the final output stage into clipping without distortion as well. This allows me to drive channels and busses very hard without worrying about distortion coming through, so long as I scale my master output fader to make sure no signal clips as it goes to the DAC.Also on the topic of higher sample rates, while the engineering side of things has been nicely covered, I will dig into the psychoacoustic side. There is a very basic reason why higher sample rates CAN translate into a more detailed sound. Harmonic interference from other sounds above the limit of the nyquist theory at 44.1KHz can "create" their own sounds in the audible spectrum.Also it is nice for things like reverbs and other processing as a more detailed signal produced at higher sample rates can sound more pleasing and "smooth" than one produced at lower sample rates.quote:Is a musical signal only definable by its frequency, or does it also have tonal qualities? Tonal qualities are another way of describing the frequency content of a sound. Even playing a pure tone 11.025 KHz sine wave through your speakers, there is a tonality imparted by your speakers that means you are not only hearing the 11.025 usually, but also harmonics of it as well.Those harmonics, and how they relate to each other in terms of amplitude are what determine the tonal qualities of an instrument or sound. A fairly pure tone instrument like a flute, has very weak harmonics in comparison to something like a guitar.Ok think I covered everything I wanted to so far. Not a bad article in general though, I may use it as a reference to help me teach some of these basics to others, particularly because it doesn't jump right into the correct terminology which can of course scare folks off. Seablade
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:05 am
quote:Does anyone else think the graphic representation of the circular pressure waves from the guitar is backwards? I would expect the amplitude of the sine wave to be higher closer to the source and lower the further away it gets. Am I off base? Heh knew I missed one, you are correct. Sound will get logarithmically weaker the more distant from the source. How this will happen depends on the prorogation characteristics of the signal and frequency, and the medium it is traveling through. Seablade
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:08 am
quote:Originally posted by Flawed:Let me ask a silly question. Sound is really just a wave right? Waves can be graphed using mathematical functions right? Seems like there might be some mathematical function that can represent a given frequency. Given that, couldn't there be a method of representing analog sound digitally but without the stair steps? Not that it might have a practical purpose but from a theoretical standpoint it seems interesting. Yes I have thought along those lines for a while. And some instruments(Synths etc) use similar methods to create their sounds. Of course to record at that requires a new method for ADC than the sampling techniques we have available at the moment, and reproducing would likely take more processing power than is feasible at the moment(Wild guess there), but it is something to look to the future for. Seablade
Flawed
Ars Tribunus Angusticlavius
Tribus: Still posting from a computer in Best Buy aisle 13.
Registered: Jan 14, 2000Posts: 9658
Posted: Wed Sep 19, 2007 8:08 am
quote:Originally posted by redleader:This is what PCM does. There aren't really steps. Theres a series of functions (sincs, which are a decaying sin wave) which are superimposed to create the output waveform. As you suspect, a finite number of these can be combined to produce a (finite bandwidth) analog waveform exactly.No, it has immense practical applications. Shannon's work in this area was one of the most important contributions to engineering of all time. Good to know I'm not an idiot. And I just meant for musical purposes. Not engineering important stuff!
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:11 am
quote:Originally posted by redleader:quote:Originally posted by Flawed:Given that, couldn't there be a method of representing analog sound digitally but without the stair steps? This is what PCM does. There aren't really steps. Theres a series of functions (sincs, which are a decaying sin wave) which are superimposed to create the output waveform. As you suspect, a finite number of these can be combined to produce a (finite bandwidth) analog waveform exactly. Not exactly. PCM attempts to reproduce, but the point is PCM is only accurate at certain point in the time domain, and even then of limited accuracy due to how ADC works and bit depth limitations. What he is describing is rebuilding an analog waveform from a formula so that all possible values are given.While PCM approximates this, it is not perfect, which is why there is a difference between 44.1kHz and 96kHz. Seablade
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:15 am
quote:Originally posted by JohnsonJohnson:.... Nice post. Better job of explaining things than mine. Seablade
redleader
Ars Legatus Legionis
Registered: Mar 18, 2000Posts: 26072
Posted: Wed Sep 19, 2007 8:37 am
quote:Originally posted by DRVSVS:Is a musical signal only definable by its frequency, or does it also have tonal qualities? Frequency is one basis a sound can be expressed in. But theres an infinate number of possible basis sets one could use. Frequency is the most common though, and one of the most natural for audio, since our ears are basically frequency domain detectors. quote:Originally posted by DRVSVS: Are there differences between a saw, sine and square waves at a given frequency? Are these kind of waves used in music, and if so, where? If one is dealing with frequency, then all sounds are combinations of sin waves, including square and saw. Square and saw waves aren't really relevant, except as test signals when debugging. quote:Originally posted by DRVSVS:Is there a difference between a sine and a saw wave of 11.025 kHz sampled at CD quality, when the wave is running in perfect sync with the ADC? Sure, they're completely different sounds. What you need to remember, is that frequency is the rate at which a function repeats, and also physical quantity. A square wave repeating at 10Hz contains many, many different frequencies. Infinitely many. Sin + Cos are the only functions that contain one frequency equal to their repetition rate. quote:Originally posted by DRVSVS:If the wave is out of sync to the ADC, will the amplitude of the signal be preserved? If the frequency of the signal is slightly above or below 11.025 khZ, what happens with the signal, compared to the signal sampled in perfect sync? Theres no difference, synchronization doesn't matter. quote:Originally posted by DRVSVS:How well is the nature of a sine wave of 5.5125 khZ represented at CD quality? Its limited only by the quality of the ADC and DAC.quote:Originally posted by DRVSVS: If the sine wave should be slightly modulated (the signal having been produced by e.g. an FM synthesizer), would these slight differences be caught by sampling in CD quality? Provided the modulated frequency is below 22khz, no. A modulated signal is just another type of signal.
ifjake
Wise, Aged Ars Veteran
Registered: Jun 5, 2005Posts: 164
Posted: Wed Sep 19, 2007 8:39 am
Higher sampling rates also impact the stereo image. Your brain uses your two ears to actually localize a sound source using two ways: comparing the loudness of one to the other, and comparing the timing of one to the other, or the phasing. The first one is simple. Say a sound is louder in your left ear than your right; you will perceive that sound as coming from a direction to your left. Timing is a little trickier. Your ears are not at the same point, they're on either side of your head roughly 17 cm apart. Imagine a sound source directly in front of you. Draw a triangle between the three points of your ears and that sound source. Since it's directly in front of you it should be an isosceles triangle, thus the sound has to travel the exact same distance to get to both of your ears, and should arrive at the exact same time. As you move the sound left or right, you'll notice one side of the triangle becoming longer than the other. That means there will be a slight delay between ears, the sound arriving at one ear before the other. Now because your head is only 17 cm wide, this delay can't be any longer than the time it would take the sound to travel 17 cm, so your brain is quite adept at discerning extremely minute timing differences, smaller than your individual ears can discern for frequency. While the frequencies afforded to you by 96k or 192k are inaudible, these higher frequencies also open up more precise timing differences, which do have a noticeable impact to just about anyone in regards to sound localization. It's a combination of loudness and timing which enables you to pin point where a sound is coming from. It also gets more complicated, as the shape and material of your head and ears also colors the sound as it bends around your head, as well as having to recreate this using only two speakers out in front of you with their own inherent timing differences and such. But that's another major way sampling rate impacts audio quality.
redleader
Ars Legatus Legionis
Registered: Mar 18, 2000Posts: 26072
Posted: Wed Sep 19, 2007 8:48 am
quote:Originally posted by Seablade:quote:Originally posted by redleader:quote:Originally posted by Flawed:Given that, couldn't there be a method of representing analog sound digitally but without the stair steps? This is what PCM does. There aren't really steps. Theres a series of functions (sincs, which are a decaying sin wave) which are superimposed to create the output waveform. As you suspect, a finite number of these can be combined to produce a (finite bandwidth) analog waveform exactly. Not exactly. PCM attempts to reproduce, but the point is PCM is only accurate at certain point in the time domain, and even then of limited accuracy due to how ADC works and bit depth limitations. Well yes, PCM can only reconstruct things that you've recorded. Obviously if a sound happens before you turn on the microphone, PCM can't get it back. Is it really necessary that I point this out? For the points we care about, the ones that happen while the machine is recording, you get every single point, which is good enough. No one expects to play back a recording they just made and to hear something they'll say next week.quote:Originally posted by Seablade: What he is describing is rebuilding an analog waveform from a formula so that all possible values are given. What, even values that occurred before the recording started? That seems a little silly, and I don't get that from reading the question.quote:Originally posted by Seablade:While PCM approximates this, it is not perfect, which is why there is a difference between 44.1kHz and 96kHz. But theres only a difference between those two if the signal isn't bandlimited to 22KHz.
redleader
Ars Legatus Legionis
Registered: Mar 18, 2000Posts: 26072
Posted: Wed Sep 19, 2007 8:50 am
quote:Originally posted by ifjake:Higher sampling rates also impact the stereo image. No they don't. quote:Originally posted by ifjake: While the frequencies afforded to you by 96k or 192k are inaudible, these higher frequencies also open up more precise timing differences, which do have a noticeable impact to just about anyone in regards to sound localization. No they don't, because they are inaudible. Yourbrain isn't using anything above ~ 16KHz for localization.
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:53 am
Redleader you misunderstood my postWasn't saying anything about before recording starts.PCM is only even somewhat accurate at the point at which the sample is taken by the ADC. Everything else is a matter of rebuilding the waveform from those particular samples by the DAC. What was being described would require an entire new way of recording as a sampling recording would not fit it at all, but then using formula(s) to recreate the track in, essentially an analog form.Not possible given our current technology limitations, but would be much more accurate than PCM and not need to rely on Nyquist at all.PCM is also limited as no truly accurate way of sampling has been determined, that is why there are debates as to which is better, PCM or DSD. But neither of them can produce an exact measurement for a sample very well. PCM tends to approximate while DSD only describes if it is higher or lower really.quote:But theres only a difference between those two if the signal isn't bandlimited to 22KHz. I could just as easily have used 22kHz and 44.1kHz But also see the above topic on the difference in the audible frequency spectrum made by using the higher sample rate than the 44.1 as well. Seablade
seablade
Ars Scholae Palatinae
Registered: Oct 20, 2005Posts: 845
Posted: Wed Sep 19, 2007 8:55 am
quote:Originally posted by redleader:No they don't, because they are inaudible. Yourbrain isn't using anything above ~ 16KHz for localization. Out of curiosity, where do you get that number from? I haven't heard that number before, and wouldn't make sense to me that the brain wouldn't use its full range of hearing to attempt to place sounds(Given that bass soudns cannot be placed very well). Seablade
254 posts • 123 ... 7 Next
Ars Technica > Forums
Jump to:
Select a forum
------------------
Hardware & Tweaking
Audio/Visual Club
Case and Cooling Fetish
CPU & Motherboard Technologia
Mobile Computing Outpost
Networking Matrix
Other Hardware
Agora Classifieds
Operating Systems & Software
Battlefront
Microsoft OS & Software Colloquium
Linux Kung Fu
Windows Technical Mojo
Distributed Computing Arcana
Macintoshian Achaia
Programmer's Symposium
The Server Room
Ars Lykaion
Gaming, Extra Strength Caplets
The Lounge
The Soap Box
The Boardroom
The Observatory
Ars Help & Feedback
Ars Subscription Member Areas
Image Galleries
Contact Us | Ars Technica
© Ars Technica 1998-2014
Powered by phpBB and...
© 2014 Condé Nast. All rights reserved
Use of this Site constitutes acceptance of our User Agreement (effective 3/21/12) and Privacy Policy (effective 3/21/12), and Ars Technica Addendum (effective 5/17/2012)
Your California Privacy Rights
The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices

